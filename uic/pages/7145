
<HTML>
<!-- <BODY BGCOLOR="#AAAAAA" TEXT="#000000" LINK="#9690CC"> -->

<BODY BGCOLOR="#FFFFFF">

<HEAD> 
<TITLE> CS 583 Spring 2005 </TITLE>
</HEAD>
<BODY>
<H1>
<center>
CS 583 - Spring 2005 (Under Construction ....)
</center>
</H1> 
<h2>
<center>
Data Mining and Text Mining
</center>
</H2>
<br> 

<H3> Course Objective and Organization </h3>
<p>This course has three objectives. First, to provide students with a sound basis in data mining tasks and techniques. Second, to ensure that students are able to read, present and critically evaluate data mining research papers. Third, to ensue that students are able to implement and to use some of the important data mining and text mining algorithms. 
<p>This course is organized in nine (9) sections, which cover all the main topics of data mining and text mining. For each topic, the instructor will first give a few introductory lectures first. Then, the class will discuss some research papers. Each class discussion starts with a paper presentation (in a seminar format) by a student assigned to read and present the paper. Two programming assignments will be given to ensure that students are able to implement and use some important data mining techniques. 

<center> <h3> Think and Ask! </h3> </center>
<p> If you have questions about any topic or assignment, DO ASK me or 
even your classmates for help, I am here to make the course 
undersdood. DO NOT delay your questions. There is no such thing as a 
stupid question. The only obstacle to learning is laziness. </p>

</p> 
<h3> General Information </h3> 
<UL>
<LI> Instructor: Bing Liu 
<ul> 
   <li> Email: me
   <li> Tel: (312) 355 1318
   <li> Office: SEO 931
</ul>
<LI> Course Call Number: 19696
<LI> Lecture times: 
     <ul>
       <li> 3:30pm - 4:45pm, Tuesday & Thursday
     </ul>
<LI> Room: 208 GH
<li> Office hours: 3:30pm - 5:00pm Monday (or by appointment)
</ul>

</UL>

<H3>Grading</H3>
<UL>
<li> Final Exam: 40%
<li> Midterm: 30% -- It consists of two parts
<ul> 
<li> Date: March 10, 2005
<li> normal sit-in paper test, covering everything before text mining.
<li> <a href="http://www.cs.uic.edu/~liub/teach/cs583-spring-05/midterm-review.doc">Midterm review questions</a>
<li> (Mar 30-31) demo the use of C4.5 and CBA (in my office).
    I will give you a dataset already in the right format with both training and 
    testing data, you are expected to run, c4.5, c4.5rule and CBA to produce the 
    error rate on the test set. The dataset does not have continuous attributes. 

</ul>
<li> Programming assignments: 20%
<li> Paper presentation: 10%
</ul>
<P>
<H3>Prerequisites</H3>
<UL>
<li> Knowledge of probability and algorithms 
<li> Knowledge of C or C++ for assignments
</ul>
<p>
<H3> Teaching materials </H3> 
<ul>
<li> Text books
  <ul>
  <li> Data mining: Concepts and Techniques, by Jiawei Han and Micheline Kamber, Morgan Kaufmann Publishers, ISBN 1-55860-489-8. 
  <li> Machine Learning, by Tom M. Mitchell, McGraw-Hill, ISBN 0-07-042807-7
  <li> Modern Information Retrieval, by Ricardo Baeza-Yates and Berthier Ribeiro-Neto, Addison Wesley, ISBN 0-201-39829-X 
  </ul>
<li> Data mining resource site: <a href="http://www.kdnuggets.com/index.html"> KDnuggets Directory </a>
</ul>
<H3> Topics (subject to change) </H3> 
<ol> 
<li> Introduction <a href="http://www.cs.uic.edu/~liub/teach/cs583-spring-05/CS583-intro.ppt"> Slides </a>
<li> Data pre-processing: data cleaning, transformation, feature selection and discretization <a href="http://www.cs.uic.edu/~liub/teach/cs583-spring-05/CS583-dataprep.ppt"> Slides </a>
<li> Association rule mining <a href="http://www.cs.uic.edu/~liub/teach/cs583-spring-05/CS583-assoc.ppt"> Slides </a>
     <ul> 
     <li> Basic concepts
     <li> Apriori Algorithm
     <li> FP-growth algorithm
     <li> Mining association rule with multiple minimum supports 
     <li> Mining class association rules
     <li> Sequential pattern mining 
     </ul>
<li> Classification (supervised learning) <a href="http://www.cs.uic.edu/~liub/teach/cs583-spring-05/CS583-classification.ppt"> Slides </a>

     <ul> 
     <li> Basic concepts
     <li> Decision trees
     <li> Naive-Bayesian classifier
     <li> Classification based on association rules
     <li> Other classification methods
     <li> Classifier evaluation
     <li> Experiment with the classification systems, C4.5 and CBA (you will write a Naive-Bayesian classifier as an assignment). As part of Midterm test, you are required to demo to me how to run C4.5 and CBA given a dataset. 
<a href="http://www.cs.uic.edu/~liub/teach/cs583-spring-05/demo.ppt"> C4.5-CBA slides</a>, <a href="http://www.cs.uic.edu/~liub/teach/cs583-spring-05/SVMLight.ppt"> SVM slides </a>
     </ul>
<li> Clustering (unsupervised learning) <a href="http://www.cs.uic.edu/~liub/teach/cs583-spring-05/CS583-clustering.ppt"> Slides </a>
     <ul> 
     <li> Basic concepts
     <li> Similarity measures
     <li> Partition method: K-mean algorithm and K-medoids algorithm
     <li> Hierarchical method: Agglomerative and divisive clustering
     <li> Density-based clustering
     <li> Clustering using a supervised learning method (decision tree)
     <li> Scale-up clustering algorithms
     </ul>
<li> Post-processing: Are all the data mining results interesting? <a href="http://www.cs.uic.edu/~liub/teach/cs583-spring-05/CS583-postproc.doc"> Slides </a>
     <ul>
     <li> Objective interestingness
     <li> Subjective interestingness
     </ul>
<li> Text mining <a href="http://www.cs.uic.edu/~liub/teach/cs583-spring-05/CS583-textMining.ppt"> Slides </a>
     <ul>
     <li> Basic text processing and representation
     <li> Intruduction to information retrieval
     <li> Text classification
          <ul> 
          <li> Rocchio method
          <li> Naive-Bayesian classifier (for texts)
          <li> K-Nearest Neighbor
          <li> Support vector machines
          <li> Experiment with the SVMlight system. 
          </ul>
     <li> Text clustering
     </ul> 
<li> Partially supervised learning <a href="http://www.cs.uic.edu/~liub/teach/cs583-spring-05/CS583-semi-supervised-learning.ppt"> Slides <a>
     <ul>
     <li> Learning with a small set of labeled and a large set of unlabeled data
     <li> learning with positive and unlabeled data
     <li> experiment with the LPU system. 
     </ul>
<li> Introduction to Web mining: Search, information extraction and integration, Web log mining, personalization and recommendation. 
<li> Summary
</ol>
<H3> Programming Projects - graded  (you will demo both programs to me at the same time)</H3>
<p> For both programming projects, you should use exactly the same file format as C4.5 and CBA. 
<ol>
<li> Implement a naive bayesian classifier:
<ul>
<li> Input: input files format should be the same as C4.5 and CBA. 
<li> Output: given a training dataset and the test set, your program 
     should output a confusion matrix and also the accuracy on the
     test set. 
<li> You can use C4.5 or CBA's example datasets for testing.
<li> You can ignore any missing values as we discussed in class.
<li> You MUST not read in the whole dataset into memory. Instead,
     you should read one tuple at a time and do your computation,
     and then read in the next (replacing the previous one in memory).
<li> Regarding smoothing, please see this <a href="http://www.ics.uci.edu/~pazzani/Publications/mlj97-pedro.pdf">paper (page 107)</a>
<li> If a dataset has continuous attributes, you can use CBA's 
     discretizer to discretize these attributes. For your demo, the dataset 
     that I give you has no continuous attribute. 
<li> You can use any programming language you want. If the language
     that you use is not available on the department machine
     (bert.cs.uic.edu). You need to bring your laptop 
     to my office to demo to me. 
<li> <b>Deadline: Mar 28, 2005</b>. 
</ul> 
<li> Implement the k-mean algorithm
<ul>
<li> Input: input file format should be the same as C4.5 and CBA. 
     We assume that every attribute is continuous. There is
     no missing value. Use Euclidean distance as the similarity 
     measure. 
<li> Output: Print the centriod and the number
     of data points in each cluster. Store the actual data points of 
     each cluster in a separate file. 
<li> You MUST make sure that one can manually select a few data 
     points as the seeds. That is, I can tell your program to 
     use some data points as the initial seeds, e.g., data point 
     1, 5, 3, 8. 
<li> You can use some continuous data from CBA's example datasets 
     for testing. You should also make up a small dataset to check
     whether your clustering is correct. 
<li> You can use any programming language you want. If the language
     that you use is not available on the department machine
     (bert.cs.uic.edu). You need to bring your laptop 
     to my office to demo to me later. 
<li> <b>Deadline: Mar 28, 2005</b>. 
</ul>
</ol>

<H3> Paper presentation - graded </H3> 
<ul> 
<Li>Students will read research papers and present the main ideas or
techniques of the papers in the class. 
<li>Students will read papers in groups. Each group have two students and will focus on one paper. Each group will also do a presentation on the paper. Both students in the group are expected to share the presentation and to answer questions during the presentation. Each presentation will last 35 minutes (30 minutes talk and 5 minutes questions & answers)
<li> Please form your own group by Mar 8, 2005
<li> Presentation schedule: The presentation starts on Mar 31 2005. We follow the sequence of papers in the Web page. Two groups will present in each class. Prepare your PPT slides. Depending on your talking speed, the number of your slides should be no more than 35. 
 <ul>
 </ul>
</ul>
<br>
<H3> Rules and Policies </H3> 
<uL>
<li> <b>Statute of limitations</b>: No grading questions or complaints, no matter how justified, will be listened to one week after the item in question has been returned. 
<li> <b>Cheating</b>: Cheating will not be tolerated. All work you submitted must be entirely your own. Any suspicious similarities between students' work (this includes, exams and program) will be recorded and brought to the attention of the Dean. The MINIMUM penalty for any student found cheating will be to receive a 0 for the item in question, and dropping your final course grade one letter. The MAXIMUM penalty will be expulsion from the University. 
<li> <b>MOSS</b>: Sharing code with your classmates is not acceptable!!! All programs will be screened using the Moss (Measure of Software Similarity.) system. 
<li> <b>Late assignments</b>: Late assignments will not, in general, be accepted. They will never be accepted if the student has not made special arrangements with me at least one day before the assignment is due. If a late assignment is accepted it is subject to a reduction in score as a late penalty. 

</ul>
Back to <a href="http://www.cs.uic.edu/~liub">Home Page</a>.
<ADDRESS>
By Bing Liu, Dec 8 2004.
</ADDRESS>
</BODY>
</HTML>
