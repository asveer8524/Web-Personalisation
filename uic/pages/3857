<HEAD>
<META NAME="allow-search" content="YES">
<META NAME="searchtitle"  content="LPU S-EM Spy-EM Expectation Maximization 
[Bing Liu, Wee Sun Lee, Philip Yu and Xiaoli Li, S-EM, SVM, Lpu, Spy-EM, Spy EM, SEM, sem, NB, nb, text mining, Web mining, Web content mining, 
text Classification, text learning, text categorization, partially supervised learning, Naive Bayesian Classifier, Lidstaone smoothing, lidstone correction, laplace correction, semi-supervised learning, learning with positive and unlabeled data, machine learning, Classification, Estimators, Leave-one-out, 
Estimator, Algorithm, Learning, Training] by webmaster@cs.uic.edu">
<META NAME="keywords" CONTENT="Bing Liu, Wee Sun Lee, Philip Yu and Xiaoli Li, LPU, lpu, SVM, S-EM, SEM, NB, sem, nb, Text Classification, Classification, Estimators, Leave-one-out, Estimator, Algorithm, Learning, Machine Learning, Training">
<META NAME="description" CONTENT="S-EM positive and unlabelled learning, classification">
<META NAME="page-type" CONTENT="Bing Liu, Wee Sun Lee, Philip Yu and Xiaoli Li,, SEM S-EM, Spy EM Spy-EM, Expectation Maximization, Text Classification, Training naive bayesian classifier, positive and unlabelled, postive and unlabeled, no negative data documents, document classification, document categorization, naive bayes classification, Text mining, Classification, categorization, Estimators, Leave-one-out, Estimator, Algorithm, Learning, Machine Learning, Training">
<META NAME="revisit-after" CONTENT="14 days">
<META NAME="audience" CONTENT="All">
<META NAME="author" content="Bing Liu">
<META NAME="abstract" CONTENT="S-EM SEM positive and unlabeled learning">
<BASE TARGET="Main">
<TITLE> LPU download page </TITLE>
<BODY bgcolor="#f5f5f5">
<H2> <center> LPU: <i>L</i>earning from <i>P</i>ositive and <i>U</i>nlabeled Examples </center></H2>
<H1> <center> LPU Download and Help </center> </h1>
<br><h3><img border="0" src="http://www.cs.uic.edu/~liub/new.gif" width="45" height="20"> New Book: <a href="http://www.cs.uic.edu/~liub/WebMiningBook.html">
<font color=#ff0000><b>Web Data Mining</b> - Exploring Hyperlinks, 
Contents and Usage Data</font></a> </h3>
<p> <b>Recent <A HREF="http://www.cs.uic.edu/~liub/LPU/PU-learning.ppt"><font color=#ff0000>talk</font></a> given at Boeing and UIUC, which summarizes the theory and some algorithms.</b>  <br><br><br>

<p>
LPU (which stands for Learning from Positive and Unlabeled data) is a text learning or 
classification system that learns from a set of positive documents and a set of unlabeled documents (without labeled negative documents). 
This type of learning is different from classic text learning/classification, in which both positive and negative training 
documents are required. 
<p> Given a set of positive documents and a set of unlabeled documents, the LPU algorithm learns a classifier in two steps:
<ul>
<li> Step 1: Identifying a set of reliable negative documents from the unlabeled set. For this step, LPU has three techniques, i.e., spy, roc (rocchio), nb (naive bayes). In all these techniques, the unlabeled set is treated as negative data. 
<p>
<li> Step 2: Building and selecting a classifier, which consists of two sub-steps:
<p>
<ol>
<li> Building a set of classifiers by iteratively applying a classification algorithm. For this step, LPU has two techniques, SVM and EM (Expectation Maximization). 
<p>
<li> Selecting a good classifier from the set of classifiers constructed above. We call this sub-step "catching a good classifier". We have a few catch mechanisms. The one in the last paper (ICML-03) below is still under extensive test in the context of the LPU system and is thus not included in the current version of the system. 
</ol>
</ul>
<p> The first two steps together can be seen as an iterative method of increasing 
the number of unlabeled examples that are classified as negative 
while maintaining the positive examples correctly classified. This strategy 
closely follows the theory given in the third paper (ICML02) below. 
The first three papers below will give you more ideas. The
first paper (ICDM03) summarizes the approaches and gives a detailed description of
the LPU system. It also proposes a biased SVM formulation to solve the problem. The system for the last paper (ICML03) can also be downloaded (see the link following the paper), which proposes a weighted logistic regression technique. However, we have not compared LPU and the biased SVM with this logistic regression based method. We are doing it now. 
<ol>

<p><li> Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee and and Philip Yu. <A HREF="http://www.cs.uic.edu/~liub/publications/ICDM-03.pdf">Building Text Classifiers Using Positive and Unlabeled Examples</a>. To appear in <i> Proceedings of the Third IEEE International Conference on Data Mining (ICDM'03)</i>, Melbourne, Florida, November 19-22, 2003.
<p><li> Xiaoli Li, Bing Liu. <A HREF="http://www.cs.uic.edu/~liub/publications/ijcai03-textClass.pdf">Learning to classify text using positive and unlabeled data.</a> <i>Proceedings of Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03)</i>, Aug 9-15, 2003, Acapulco, Mexico.
<p><li> Bing Liu, Wee Sun Lee, Philip S Yu and Xiaoli Li. <A HREF="http://www.cs.uic.edu/~liub/S-EM/unlabelled.pdf">Partially Supervised Classification of Text Documents.</a> <i>Proceedings of the Nineteenth International Conference on Machine Learning (ICML-2002)</i>, 8-12, July 2002, Sydney, Australia.
<p><li> Wee Sun Lee, Bing Liu. <A
HREF="http://www.cs.uic.edu/~liub/publications/icml-03.pdf">Learning with Positive and Unlabeled Examples using Weighted Logistic Regression</a>. 
<i> Proceedings of the Twentieth International Conference on 
Machine Learning </I>(ICML-2003), August 21-24, 2003, Washington, DC USA. <a href="http://www.comp.nus.edu.sg/~leews/publications/logistic.tar.gz"> Source code </a>
</ol>
<br>
<h2> Executable (.exe) </h2>
<p>
Currently, we only provide executable (.exe) version of the system (without source) which runs on Windows PC. The program is free for scientific use. Please contact us, if you are planning to use the software for commercial purposes. The software must not be distributed without prior permission of the 
authors. 
<br>
<br><br>
<H2> Download and Install </h2>

<ol>
<li>Download the LPU program <A HREF="http://www.cs.uic.edu/~liub/LPU/lpu.zip"> here </a>
<li>Extract the files in the zip file to a directory. In this directory, 
the LPU directory will be created, which contains 4 files. lpu.exe is 
the executable program. The other three files show an example dataset. 
<li> You also need to download Thorsten Joachims's SVMlight from his Web site <A href="http://download.joachims.org/svm_light/current/svm_light_windows.zip"> SVMlight Windows version</a>. This is a zip file. You can extract and put the two .exe files (svm_classify.exe and svm_learn.exe) in the LPU directory. After that, you are ready to experiment.  
</ol>
<p> If you have downloaded LPU, <a href="http://www.cs.uic.edu/~liub">Please send us an email</a> so that we can put you in our mailinglist to inform 
you any new versions and bug-fixes. 

<br><br>
<br>
<h2> How to use </h2>
<p> Open a DOS Window (Command Prompt) from your PC and go to the 
LPU directory. You can run the system from there. The data files
must be in the LPU directory. You can use the following command to run:
<p> 
lpu -s1 [option 1] -s2 [option 2] -c [option 3] -f [filestem]
</p>
<dir><pre>
-s1: represent step 1
-s2: represent step 2
-c:  classifier selection method, also called catch method. 
Option 1: technique used for step 1. It can be one of the three: spy, roc, nb
option 2: technique used for step 2. It can be one of the two: svm, em. 
option 3: technique for selecting the final clasifier. It can be one of the 
          two: 1, 2. Method 1 is the method used in the IJCAI-03 paper above, 
          which selects the first or the last classifier as the final 
          classifier. Method 2 is a new method (not published), which can catch 
          a classifier in the middle and tend to produce better results. 
(The first two papers above gives you a good idea about these options.)

Some examples,
   For example, the filestem of the dataset is "demo" (each dataset consists
        of three input files, e.g., demo.pos, demo.unlabel and demo.test. 
        See the Input files below for details). This dataset is included in 
        the download zip file.
   To run LPU, if you want to use the spy technique for step 1, the svm 
   technique for step 2 and the second method for classifier selection, 
   you use this command:

      lpu -s1 spy -s2 svm -c 2 -f demo

   If you want to use the spy technique for step 1, and the em technique 
   for step 2 (please do not use -c option for em as the classifier selection 
   method used in em is automatically applied), you use this command:

      lpu -s1 spy -s2 em -f demo
   
   (This combination is exactly our earlier technique, S-EM)

   If you want to use the rocchio technique for step 1, the SVM technique 
   for step 2, and method 1 for classifier selection, you use this command:

      lpu -s1 roc -s2 svm -c 1 -f demo
   
   (This combination is the technique in our IJCAI-03 paper without clustering)
</pre></dir>
<br>
<h2> Input Files (three files for each dataset) </h2>
<pre> 
    filestem.pos
    filestem.unlabel
    filestem.test

filestem.pos: It contains all the positive training data (or documents). 
filestem.unlabel: It contains all the unlabeled data.
filestem.test: It contains all the test data. Positive documents should
   have target +1 and negative documents should have target -1 (see below also)
</pre>
<p> Note that the LPU system can be used for retrieval or classification. 
For retrieval, the document collection is the unlabeled set, which is
also the test set. For classification, you can provide a separate test set
that is different from the unlabeled set used in training. 
</p>
<br>
<H2> Data Format </H2>
<p> 
Each line represents a document. 
<pre>
line    =: target feature:value feature:value ... feature:value
target  =: +1 | -1 | 
feature =: integer
value   =: integer
</pre>
<p>
The target value and each of the feature:value pairs are separated 
by a space character. Each feature (keyword) is represented with an
integer (the first feature of your dataset MUST BE 1), and its value 
is the number of times (frequency count) that the feature (keyword) appeared 
in the document. Features with value zero can be skipped. The feature 
number in each document must be in increasing order, i.e., "34:2 356:4" is
ok, but not "356:4 34:2"
</p>
<pre>
In filestem.pos, no target value should be specified.
   E.g., 
   34:2 356:4
   365:3 460:5

In filestem.unlabel, no target value should be specified.
   E.g., 
   34:2 356:4
   365:3 460:5

In filestem.test, the target value of each document is +1 or -1 
         according to its class. 
   E.g., 
   -1 34:2 356:4
   +1 365:3 460:5

One "demo" dataset with three files is included in the downloaded zip file. 

NOTE: When running SVM, the feature counts are automatically converted
      to normalized tf-idf values by LPU.

</pre>

<br>
<h2> Output </h2> 
<ul>
<li> The results (precision, recall, F score, accuracy) 
           of each iteration of SVM or EM is output on the screen. 
           The final set of results is the result of LPU which is caught
by our catch mechanism. Many times, 
           you will see that some SVM or EM iterations produce better 
           results. Well, it is very hard to catch the best. All the results
are obtained from the test data that you provided. 

<li> The ourput formats are a little different for SVM and EM just to 
           distinguish them. However, the results are easy to see.  

</ul>           
<HR>

<!-- Start of StatCounter Code -->
<script type="text/javascript" language="javascript">
var sc_project=2031920; 
var sc_invisible=1; 
var sc_partition=18; 
var sc_security="1d4151a9"; 
var sc_remove_link=1; 
</script>

<script type="text/javascript" language="javascript" src="http://www.statcounter.com/counter/counter.js"></script><noscript><img  src="http://c19.statcounter.com/counter.php?sc_project=2031920&amp;java=0&amp;security=1d4151a9&amp;invisible=1" alt="counter free hit unique web" border="0"> </noscript>
<!-- End of StatCounter Code -->


<p>Created on July 10 2003 by <a href="http://www.cs.uic.edu/~liub">Bing Liu</a>; and 
<a href="http://www.comp.nus.edu.sg/~lixl">Xiaoli Li</a>; 
We thank Sarah Zhai for carrying out a large number of tests on the system. 
</body>
