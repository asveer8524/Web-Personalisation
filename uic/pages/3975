
<!-- saved from url=(0058)http://www.cs.uic.edu/~liub/diagnosticDM/diagnosticDM.html -->
<html><script type="text/javascript">window["_gaUserPrefs"] = { ioo : function() { return true; } }</script><head>
<meta name="allow-search" content="YES">
<meta name="searchtitle" content="lifelong learning, lifelong machine learning, data mining, big data
[Bing Liu, Zhiyuan Chen, machine learning, topic model, LDA, LTM, AMC] by webmaster@cs.uic.edu">
<meta name="keywords" content="Lifelong Learning, Lifelong Topic Model, Big Data, Topic Model, Knowledge-based Topic Model, Data Mining, Text Mining">
<meta name="description" content="Lifelong Machine Learning and Big Data">
<meta name="page-type" content="Lifelong Machine Learning and Big Data">
<meta name="revisit-after" content="14 days">
<meta name="audience" content="All">
<meta name="author" content="liub">
<meta name="abstract" content="Lifelong Machine Learning and Big Data">
<!--<base target="Main">--><base href="." target="Main">
<title> Lifelong Machine Learning </title>
</head><body bgcolor="#f5f5f5">

<h1> <center>Lifelong Learning - learn as "humans do" </center></h1>
<h3> <center>(also known as <i>Continual Machine Learning</i> or <i>Continuous Machine Learning</i>) <br>Accumulate knowledge learned in the past and <br>use it to learn more knowledge to build a lifelong learning machine<br> 
to achieve Artificial General Intelligence (AGI)
</center> </h3>
<br>
<b><span style="font-size:20pt"><font color=#ff0000>Second Edition:</font>
"<a href="https://www.cs.uic.edu/~liub/lifelong-machine-learning.html">Lifelong Machine Learning</a>."
by Z. Chen and B. Liu, Morgan & Claypool, August 2018 (1st edition, 2016). </span></b><br/>
<li>Added three new chapters: (4) <a href="http://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf">Continual Learning and Catastrophic Forgetting</a>, (5) <a href="http://www.cs.uic.edu/~liub/lifelong-learning/open-world-learning.pdf">Open-world Learning</a>, (8) <a href="http://www.cs.uic.edu/~liub/lifelong-learning/lifelong-learning-in-dialogues.pdf">Continuous Knowledge Learning in Chatbots</a>
<li> Introduced the concept of <i>learning on the job</i> or <i>learning while working</i>. </li> 
<li> Updated and/or reorganized the other chapters.</li>
<br>
<b><span style="font-size:14pt"><a href="http://www.cs.uic.edu/~liub/Lifelong-Machine-Learning-Tutorial-KDD-2016.pdf">Lifelong Machine Learning Tutorial</a>. Title: lifelong machine learning and computer reading the Web, KDD-2016, August 13-17, 2016, San Francisco, USA. </span></b><br/>
<br>
<b><span style="font-size:14pt"><a href="http://www.cs.uic.edu/~liub/IJCAI15-tutorial.html">Lifelong Machine Learning Tutorial</a>, IJCAI-2015, July 25-31, 2015, Buenos Aires, Argentina.</span></b><br/>
<br> 
<b><span style="font-size:14pt">A Podcast: "<a href="http://worldofpiggy.com/podcast/2017/03/28/lifelong-machine-learning/">Machines that Learn Like Humans</a>" by my former student Zhiyuan Chen and Francesco Gadaleta (host).</span></b><br/>

<br>
<table> 
<tr>
<td width="50%" valign=top>
<b><span style="font-size:14pt"><a href="http://lifelongml.org/">A Resource Site</a> maintained by Eric Eaton's group</span></b><br/> 
</td>
<td width="50%" valign=top>
<b><span style="font-size:14pt">DARPA new program <a href="http://www.darpa.mil/news-events/2017-03-16">Lifelong Learning Machines (L2M)</a>, 3/16/2017</span></b><br/>
</td>
</tr>
</table>
<p>
Statistical learning algorithms like deep NN, SVM, HMM, CRF, and topic 
modeling have been very successful in machine learning and data mining 
applications. Given a dataset, such an algorithm simply runs on the 
dataset to produce a model without considering any 
related information or past learning results. Although these algorithms 
can still be improved, such single task and isolated algorithmic 
approaches to machine learning have their limits, e.g., it needs a large 
number of training examples and is only suitable for well-defined and 
narrow tasks in closed environments. Looking ahead, the <b>question</b> is how to deal with 
these limitations to move machine learning to the next phase. I believe 
the answer is <b><i>lifelong machine learning</i></b> or simply 
<b><i>lifelong learning</i></b> (also called <b><i>continual learning</i></b> or even <b><i>continuous learning</i></b>), which tries to mimic "human learning" (we don't know how humans learn) to build a <i>lifelong learning machine</i>. The key characteristic of "human learning" is the <i>continual learning and 
adaptation to new environments</i> - we retain or accumulate the knowledge 
gained from past learning and use the knowledge to help future learning 
and problem solving with possible adaptations. Ideally, it should also be 
able to discover new tasks and learn on the job in open environments in
a self-supervised manner.
Existing isolated learning algorithms are not capable of doing that. 
However, without the lifelong learning capability, AI systems will 
probably never be truly 
intelligent. We believe that now is the right time to explore lifelong 
learning. <b>Big data</b> offers a golden opportunity 
because its large volume and diversity give us abundant information for discovering rich 
and commonsense knowledge automatically, which can enable an lifelong
learning machine or agent to <i>continually learn and 
accumulate knowledge</i>, and to become more and more 
knowledgeable and better and better at learning. 

<p> <b>Human learning is very different</b>: I believe that no human being has ever been given 1000 positive and 
1000 negative documents (or images) and asked to learn a text classifier. 
As we have accumulated so much knowledge in the past and understand it, 
we can usually learn with little effort and few examples. If we don't 
have the accumulated knowledge, even if we are given 2000 training 
examples, it is very hard to learn manually. For example, I don't 
understand Arabic. If you give me 2000 Arabic documents and ask me to 
build a classifier, I cannot do it. But that is exactly what the current 
machine learning algorithms do. That is not how humans learn.

<h3> Related Learning Paradigms: Transfer learning, multitask learning, and lifelong learning</h3>
<ul>
<li> <b>Characterisitcs of lifelong learning</b>: (1) learning continuously (ideally in the open world), (2) accumulating the previously learned knowledge to become more and more knowledgeable, (3) using the knowledge to learn more knowledge and adapting it for problem solving, (4) discovering new problems/tasks to be learned and learning them incrementally, and (5) learning on the job or learning while working, improving model during testing or model applications.  
<li> <b>Transfer learning vs. lifelong learning</b>: Transfer learning 
uses the source domain labeled data to help target domain learning. 
Unlike lifelong learning, transfer learning is not continual and has 
no knowledge retention (as it uses source labeled data, not learned 
knowledge). The source must be similar to the target (which 
are normally selected by the user). It is also only one-directional: 
source helps target, but not the other way around because the target has no
or little labeled data. 

<li><b>Multitask learning vs. lifelong learning</b>: Multitask learning 
jointly 
optimizes learning of multiple tasks. Although it is possible to make 
it continual, multitask learning does not retain any explicit knowledge 
except data, and when the number of task is really large, it is hard to 
re-learn everything when faced with a new task.
</ul>
<h3>Our Work</h3>

Much of my work uses tasks in <a href="https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html">sentiment analysis</a> (SA) (an area of NLP) as applications because SA has been my main research interest. In fact, it is the problems that I encountered in a SA startup that strongly motivated me to work on lifelong learning. Besides, online reviews provide excellent data for lifelong learning. 

<ul><li> <b>Lifelong Unsupervised Learning</b>: 
<!--  (<a href="http://www.cs.uic.edu/~zchen/papers/ICML2014-Zhiyuan(Brett)Chen.pdf">ICML-2014</a>, <a href="http://www.cs.uic.edu/~zchen/papers/KDD2014-Zhiyuan(Brett)Chen.pdf">KDD-2014</a>, <a href="http://www2016.net/proceedings/proceedings/p167.pdf">WWW-2016</a>, <a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11973/12051">AAAI-2016</a>, <a href="http://www.cs.uic.edu/~liub/publications/emnlp2016.pdf">EMNLP-2016)</a></b>: -->
<ul>
<li> <b><i>Lifelong topic modeling</i></b> (<a href="http://www.cs.uic.edu/~zchen/papers/ICML2014-Zhiyuan(Brett)Chen.pdf">ICML-2014</a>, <a href="http://www.cs.uic.edu/~zchen/papers/KDD2014-Zhiyuan(Brett)Chen.pdf">KDD-2014</a>, <a href="\http://www2016.net/proceedings/proceedings/p167.pdf">WWW-2016</a>):
retain and consolidate the topics (knowledge) learned from previous domains and uses the knowledge suitably for future modeling in other domains. <!-- This paradigm is powerful because different domains or tasks share a great deal of concepts or topics, which can be exploited to generate much better topics for the new task. -->
<li> <b><i>Lifelong belief propagation</i></b> (<a href="http://www.cs.uic.edu/~liub/publications/emnlp2016.pdf">EMNLP-2016</a>): use the knowledge 
learned previously to expand the graph and to obtain more accurate prior 
probabilty distributions.
<li> <b><i>Lifelong information extraction</i></b> (<a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11973/12051">AAAI-2016</a>): make use of previously learned knowledge for better extraction. 
</ul>
<li><b>Lifelong Supervised Learning</b> (<a href="http://www.cs.uic.edu/~zchen/papers/ACL2015-Short-Zhiyuan(Brett)Chen.pdf">ACL-2015</a>, <a href="http://www.cs.uic.edu/~liub/publications/acl17_LifelongCRF.pdf">ACL-2017</a>): 
<ul>
<li><b>Using a generative model</b> (<a href="http://www.cs.uic.edu/~zchen/\
papers/ACL2015-Short-Zhiyuan(Brett)Chen.pdf">ACL-2015</a>): The ACL-2015 work is about lifelong learning using a generative model. It is used for sentiment classification.
<li> <b>Learning during testing or after model building</b> (<a href="https://www.cs.uic.edu/~liub/publications/acl17_LifelongCRF.pdf">ACL-2017</a>): This ACL-2017 work is about learning more during testing based on model application results. It is like <i>learning on the job</i>. The idea is applied to an information extraction task. 
</ul>
<li><b><a href="http://www.cs.uic.edu/~liub/open-classification.html">Open world Learning</a> (a.k.a. open world classification or open classification)</b> (<a href=""http://www.kdd.org/kdd2016/papers/files/rpp0426-feiA.pdf">KDD-2016</a>, EMNLP-2017): this learning paradigm is becoming very important as AI agents (e.g., self-driving cars and chatbots)
are increasingly facing the real-world open and dynamic environments, where there are always new or unexpected things. 
But traditional learning makes the close-world assumption: test instances must be from only the training/seen classes, which is not true in the open world. 
Ideally, an open-world learner should be able to do the following:
<ul>
<li> detecting instances of unseen classes - not seen in training (the <b>DOC algorithm</b> (<a href="https://www.cs.uic.edu/~liub/publications/emnlp17-camera-ready.pdf">EMNLP-2017</a>) is very powerful for this task for both text and images), 
<li> autmatically identifying unseen classes from the detected instances, and
<li> incrementally learning the new/unseen classes. 
</ul>
In this process, the system accumulates more and more knowledge. 
This is <b><i> open world machine learning</i></b> because it does not make 
the traditional <b><i>closed world assumption</i></b> that test instances must be from classes seen in training.  
in training). The learner is <i>self-supervised</i> and <i>it knows what it does and does not know.</i>
<li><b> <href="http://www.cs.uic.edu/~liub/continuous-learning-chatbot.html">Continuous learning in chatbots</a> (<a href="https://arxiv.org/abs/1802.06024">2018 paper</a>)</b>: Chatbots have been very popular in recent years, but they cannot learn new knowledge during the conversation process, i.e., their knowledge is fixed beforehand and cannot be expanded during conversations. In this work, we aim to build a lifelong knowledge learning engine for chatbots. 
</ul>

<h3> Publications </h3>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>TextBook</b>: Zhiyuan Chen and Bing Liu. <a href="https://www.cs.uic.edu/~liub/lifelong-machine-learning.html">Lifelong Machine Learning</a>. Morgan & Claypool, 2018 (2nd edition), 2016 (1st edition). 

<ul>

<p><li>Shuai Wang, Guangyi Lv, Sahisnu Mazumder, Geli Fei, and Bing Liu. Lifelong Learning Memory Networks for Aspect Sentiment Classification. To appear in <i>Proceedings of 2018 IEEE International Conference on Big Data (IEEE BigData 2018)</i>, Seattle, December 10-13, 2018. 

<p> <li> Lei Shu, Hu Xu, and Bing Liu. 
<a href="https://arxiv.org/abs/1801.05609">Unseen Class Discovery in Open-world Classification</a>. arXiv:1801.05609 [cs.LG], 18 Jan. 2018. 

<p> <li> Sahisnu Mazumder, Nianzu Ma, and Bing Liu.  
<a href="https://arxiv.org/abs/1802.06024">Towards a Continuous Knowledge Learning Engine for Chatbots</a>. arXiv:1802.06024 [cs.CL], 16 Feb. 2018. <b>Previous title</b>: "<a href="https://arxiv.org/abs/1802.06024">Towards an Engine for Lifelong Interactive Knowledge Learning in Human-Machine Conversations</a>".


<p> <li> Hu Xu, Bing Liu, Lei Shu and Philip S. Yu. Lifelong Domain Word Embedding via Meta-Learning. <i>Proceedings of International Conference on Artificial Intelligence (IJCAI-ECAI-2018)</i>. July 13-19 2018, Stockholm, Sweden. 

<p> <li> Lei Shu, Hu Xu, Bing Liu. <a href="http://www.cs.uic.edu/~liub/publications/emnlp17-camera-ready.pdf">DOC: Deep Open Classification of Text Documents</a>. <i>Proceedings of 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP-2017, oral presentation short paper)</i>, September 7–11, 2017, Copenhagen, Denmark.

<p> <li>Lei Shu, Hu Xu, and Bing Liu. <a href="http://www.cs.uic.edu/~liub/publications/acl17_LifelongCRF.pdf">Lifelong Learning CRF for Supervised Aspect Extraction</a>. <i>Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL-2017, oral presentation short paper)</i>, July 30-August 4, 2017, Vancouver, Canada.

<p> <li>Lei Shu, Bing Liu, Hu Xu, and Annice Kim. <a href="http://www.cs.uic.edu/~liub/publications/emnlp2016.pdf">Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets</a>. <i>Proceedings of 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-2016)</i>, November 1–5, 2016, Austin, Texas, USA.

<p> <li>Geli Fei, Shuai Wang, and Bing Liu. 2016. <a href="http://www.kdd.org/kdd2016/papers/files/rpp0426-feiA.pdf">Learning Cumulatively to Become More Knowledgeable</a>. <i>Proceedings of SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2016)</i>, August 13-17, San Francisco, USA. 

<p> <li> Geli Fei, and Bing Liu. 2016. <a href="https://pdfs.semanticscholar.org/c5b5/56807c19548384886a060216672c11121a72.pdf?_ga=1.25126456.34606428.1491067408">Breaking the Closed World Assumption in Text Classification</a>. <i>Proceedings of NAACL-HLT 2016 </i>, June 12-17, San Diego, USA.

<p> <li> Shuai Wang, Zhiyuan Chen, and Bing Liu. <a href="http://www2016.net/proceedings/proceedings/p167.pdf">Mining Aspect-Speciﬁc Opinion using a Holistic Lifelong Topic Model</a>. <i>Proceedings of the International World Wide Web Conference (WWW-2016)</i>, April 11-15, 2016, Montreal, Canada. 

<p> <li> Qian Liu, Bing Liu, Yuanlin Zhang, Doo Soon Kim and Zhiqiang Gao. <a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11973/12051">Improving Opinion Aspect Extraction using Semantic Similarity and Aspect Associations</a>. <i>Proceedings of Thirtieth AAAI Conference on Artificial Intelligence (AAAI-2016)</i>, February 12–17, 2016, Phoenix, Arizona, USA.

<p> <li> Zhiyuan Chen, Nianzu Ma and Bing Liu. <a href="http://www.cs.uic.edu/~zchen/papers/ACL2015-Short-Zhiyuan(Brett)Chen.pdf">Lifelong Learning for Sentiment Classification</a>. <i>Proceedings of the 53st Annual Meeting of the Association for Computational Linguistics (ACL-2015, short paper)</i>, 26-31, July 2015, Beijing, China. 

<p> <li> Zhiyuan Chen and Bing Liu. <a href="http://www.cs.uic.edu/~zchen/papers/KDD2014-Zhiyuan(Brett)Chen.pdf">Mining Topics in Documents: Standing on the Shoulders of Big Data.</a>. <i>Proceedings of the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2014)</i>, August 24-27, New York, USA. [<a href="https://github.com/czyuan/AMC.git"><b>Code</b></a>] [<a href="http://www.cs.uic.edu/~zchen/downloads/KDD2014-Chen-Dataset.zip"><b>Dataset</b></a>]
</p>
<p> <li> Zhiyuan Chen and Bing Liu. <a href="http://www.cs.uic.edu/~zchen/papers/ICML2014-Zhiyuan(Brett)Chen.pdf">Topic Modeling using Topics from Many Domains, Lifelong Learning and Big Data</a>. <i>Proceedings of the 31st International Conference on Machine Learning (ICML 2014)</i>, June 21-26, Beijing, China. 

<p> <li> Zhiyuan Chen, Arjun Mukherjee, and Bing Liu. <a href="https://www.cs.uic.edu/~zchen/papers/ACL2014-Zhiyuan(Brett)Chen-Latest.pdf">Aspect Extraction with Automated Prior Knowledge Learning</a>. <i>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)</i>, June 22-27, 2014, Baltimore, USA. 
</p>
</ul>
<br><p>Created on Sep 24, 2014 by <a href="http://www.cs.uic.edu/~liub">Bing Liu</a>.
</p></body></html>
