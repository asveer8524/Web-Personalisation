<HEAD>
<META NAME="allow-search" content="YES">
<META NAME="searchtitle"  content="LPU S-EM Spy-EM Expectation Maximization
[Bing Liu, Wee Sun Lee, Philip Yu and Xiaoli Li, S-EM, SVM, Lpu, Spy-EM, Spy EM\
, SEM, sem, NB, nb, text mining, Web mining, Web content mining,
text Classification, text learning, text categorization, partially supervised l\
earning, Naive Bayesian Classifier, Lidstaone smoothing, lidstone correction, l\
aplace correction, semi-supervised learning, learning with positive and unlabel\
ed data, machine learning, Classification, Estimators, Leave-one-out,
Estimator, Algorithm, Learning, Training] by webmaster@cs.uic.edu">
<META NAME="keywords" CONTENT="Bing Liu, Wee Sun Lee, Philip Yu and Xiaoli Li, \
LPU, lpu, SVM, S-EM, SEM, NB, sem, nb, Text Classification, Classification, Est\
imators, Leave-one-out, Estimator, Algorithm, Learning, Machine Learning, Train\
ing">
<META NAME="description" CONTENT="S-EM positive and unlabelled learning, classi\
fication">
<META NAME="page-type" CONTENT="Bing Liu, Wee Sun Lee, Philip Yu and Xiaoli Li,\
, SEM S-EM, Spy EM Spy-EM, Expectation Maximization, Text Classification, Train\
ing naive bayesian classifier, positive and unlabelled, postive and unlabeled, \
no negative data documents, document classification, document categorization, n\
aive bayes classification, Text mining, Classification, categorization, Estimat\
ors, Leave-one-out, Estimator, Algorithm, Learning, Machine Learning, Training"\
>
<META NAME="revisit-after" CONTENT="30 days">
<META NAME="ROBOTS" CONTENT="index, nofollow">
<META NAME="audience" CONTENT="All">
<META NAME="content-language" CONTENT="US">
<META NAME="author" content="liub">
<META NAME="abstract" CONTENT="Learning from positive and unlabeled Data">
<BASE TARGET="Main">
<TITLE> PU Learning: Learning from Positive and Unlabeled Examples </TITLE>
<BODY bgcolor="#f5f5f5">
<H2> <center> Partially Supervised Classification </center></H2>
<H1> <center> PU Learning - Learning from Positive and Unlabeled Examples </center> 
</h1>
<br><h3><img border="0" src="http://www.cs.uic.edu/~liub/new.gif" width="45" height="20"> New Book: <a href="http://www.cs.uic.edu/~liub/WebMiningBook.html">
<font color=#ff0000><b>Web Data Mining</b> - Exploring Hyperlinks, 
Contents and Usage Data</font></a> </h3>
<p> Funded by: <a href="http://www.nsf.gov"> NSF (National Science Fundation)</a>, Award No: IIS-0307239
<br>
<!-- <p> <b>Recent <A HREF="http://www.cs.uic.edu/~liub/LPU/PU-learning.ppt"><font color=#ff0000>talk</font></a> given at Boeing, UIUC, University of Notre Dame, University of 
Trento (Italy) and University of Siena (Italy) which summarizes the theory and some algorithms.</b> -->
<br>
<p>To our knowledge, the term <b>PU Learning</b> was coined in our 
<a href="http://www.cs.uic.edu/~liub/publications/ECML-05.pdf">ECML-2005 paper</a>. 
It stands for <i>positive and unlabeled learning</i>, also called <i>learning from positive and unlabeled examples</i>. 
Our first paper on PU learning was published in ICML-2002, which focused on text classification. Note that <i>Set Expansion</i> is basically an instance of PU learning. 

<p>
<b>Definition (PU Learning)</b>: Given a set of examples of an particular class P (called the positive class) and a set of unlabeled examples U, which contains both class P and non-class P (called the negative class) instances, the goal is to build a binary classifier to classify the test set T into two classes, positive and negative, where T can be U.

<!-- <p> Our first work on PU learning was in the context of text classification, which is commonly stated as follows: Given a set of 
labeled training documents of <i>n</i> classes, the system uses this 
training set to build a classifier, which is then used to classify 
new documents into the <i>n</i> classes. Although this classic model is 
important, in practice one also encounters another problem. That is, 
one has a set of documents of a particular topic or class <i>P</i> 
(positive class), and is given a large set <i>U</i> of mixed (unlabelled) 
documents that contains documents from class <i>P</i> and also other types 
of documents (negative documents). One wants to classify the documents 
in <i>U</i> into documents from <i>P</i> and documents not from <i>P</i>. 
The key feature 
of this problem is that there is no labeled negative training data, 
which makes the traditional text classification techniques inapplicable. 
This problem is termed, <i>partially supervised classification</i> (PSC). 
We also call it <i>PU-learning</i> (Learning from Positive and Unlabeled 
examples). 

<p>
The objectives of this project are to design a robust and principled
technique to solve PSC, implement a system for PSC, devise a method to
evaluate such techniques, and identify methods for determining the minimum
number of labeled documents needed to achieve the optimal accuracy in order
to reduce manual labeling efforts. The results of this
research should be widely useful because the identification of targeted
information/documents is of great value in this information age. 
-->
<p>
Our <a href="https://www.cs.uic.edu/~liub/S-EM/unlabelled.pdf"> ICML-2002 paper</a> showed theoretically that P and U provide sufficient information for learning, and PU learning can be posed as a constrained optimization problem. Some of our early algorithms are reported in (Liu et al 2003), (Lee and Liu 2003), (Li and Liu 2003), etc. 
<br>
<p>
<h4><font color=#ff0000>Read the following paper first</font>: It summarizes the main early ideas, proposed a <i>biased-SVM</i> technique, and performed a comprehensive evaluation. </h4>
<ul>
<li> Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee and and Philip Yu. "Building Text Classifiers Using Positive and Unlabeled Examples." <i> Proceedings of the Third IEEE International Conference on Data Mining (ICDM-03)</i>, Melbourne, Florida, November 19-22, 2003.  <A HREF="http://www.cs.uic.edu/~liub/publications/ICDM-03.pdf"> [PDF]</a>
</ul>
<h3> Publications </h3>
<ol>
<p> <li> Geli Fei and Bing Liu. "<a href="http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP282.pdf">Social Media Text Classification under Negative Covariate Shift.</a>" Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisboa, Portugal, 17-21 September 2015. 

<p> <li> Xiaoli Li, Lei Zhang, Bing Liu and See-Kiong Ng. <a href="http://www.cs.uic.edu/~liub/publications/ACL-2010-PU-learn.pdf">"Distributional 
Similarity vs. PU Learning for Entity Set Expansion."</a> 
In <i>Proceedings of the 48th Annual Meeting of the Association for 
Computational Linguistics (ACL-10, short paper) </i>, July 11-16, 2010. 

<p> <li> Xiaoli Li, Bing Liu and See-Kiong Ng. 
<a href="http://www.cs.uic.edu/~liub/publications/EMNLP-2010-no-negative.pdf">
Negative Training Data can be Harmful to Text Classification".</a> 
<i>Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-10)</i>. Oct. 9-11, 2010, MIT, Massachusetts, USA.

<p> <li>Xiaoli Li, Bing Liu and See-Kiong Ng. "Learning to Identify Unexpected Instances in the Test Set," <i>Proceedings of Twenth International Joint Conference on Artificial Intelligence (IJCAI-07)</i>, 2007. <A HREF="http://www.cs.uic.edu/~liub/publications/IJCAI-07.pdf"> [PDF]</a>

<p> <li>Xiaoli Li, Bing Liu. "Learning from Positive and Unlabeled Examples with Different Data Distributions." European Conference on Machine Learning <i>(ECML-05)</i>, 2005. <a href="http://www.cs.uic.edu/~liub/publications/ECML-05.pdf">[PDF]</a>

<p><li> Bing Liu Xiaoli Li, Wee Sun Lee and and Philip Yu. "Text Classification by Labeling Words." <i>Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004)</i>, July 25-29, 2004, San Jose, California. <A HREF="http://www.cs.uic.edu/~liub/publications/aaai04-label-words.pdf"> [PDF]</a>
<p> <li> Xiaoli Li, and Bing Liu. "Dealing with Different Distributions in Learning from Positive and Unlabeled Web Data." WWW-2004 poster paper. <A HREF="http://www.cs.uic.edu/~liub/publications/www04-pu-learn.pdf"> [PDF] </a> 
<p> <li> Gao Cong, Wee Sun Lee, Haoran Wu, Bing Liu. "Semi-supervised Text Classification Using Partitioned EM." DASFAA 2004: 482-493. <A HREF="http://www.cs.uic.edu/~liub/publications/DASFAA04.pdf"> [PDF] </a>
<p><li> Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee and and Philip Yu. "Building Text Classifiers Using Positive and Unlabeled Examples." <i> Proceedings of the Third IEEE International Conference on Data Mining (ICDM-03)</i>, Melbourne, Florida, November 19-22, 2003.  <A HREF="http://www.cs.uic.edu/~liub/publications/ICDM-03.pdf"> [PDF]</a>
<p><li> Xiaoli Li, Bing Liu. <A HREF="http://www.cs.uic.edu/~liub/publications/ijcai03-textClass.pdf">Learning to classify text using positive and unlabeled data.</a> <i>Proceedings of Eighteenth International Joint Conference
on Artificial Intelligence (IJCAI-03)</i>, Aug 9-15, 2003, Acapulco, Mexico.
<p><li> Wee Sun Lee, Bing Liu. <A
HREF="http://www.cs.uic.edu/~liub/publications/icml-03.pdf">Learning with Positive
 and Unlabeled Examples using Weighted Logistic Regression</a>.
<i> Proceedings of the Twentieth International Conference on
Machine Learning (ICML-2003)</I>, August 21-24, 2003, Washington, DC USA.
<p><li> Bing Liu, Wee Sun Lee, Philip S Yu and Xiaoli Li. <A HREF="http://www.cs
.uic.edu/~liub/S-EM/unlabelled.pdf">Partially Supervised Classification of Text
Documents.</a> <i>Proceedings of the Nineteenth International Conference on Mach
ine Learning (ICML-2002)</i>, 8-12, July 2002, Sydney, Australia.
</ol>


<h3> Software </h3>

<ul>
<li> <img border="0" src="../new.gif" width="60" height="25">
<A HREF="http://www.cs.uic.edu/~liub/LPU/LPU-download.html"> Download our latest <font color=#ff0000>
LPU</font> system, including help information </a>
<br> <br>
<LI> <A HREF="http://www.cs.uic.edu/~liub/S-EM/S-EM-download.html"> Download our 
Spy-EM <font color=#ff0000> (S-EM)</font> system</A>
--- <A HREF="http://www.cs.uic.edu/~liub/S-EM/readme.html"> Readme file</A> ---
 <A HREF="http://www.cs.uic.edu/~liub/S-EM/unlabelled.pdf">ICML-02 paper </A>
</ul>
<br>

<p> <A HREF="http://www.cs.uic.edu/~liub/NSF/IIS-0307239-8-2003-reports.htm"> NSF Grant Report</a>, Aug 5, 2003. <A Href="http://www2.cs.washington.edu/nsf2003/"> IDM 2003 Workshop</a>, September 14-16, 2003, Seattle, Washington.</p>
<br>
<h3>Acknowledgments </h3>
This project is currently suported by National Science Foundation under Grant No. IIS-0307239. Any opinions, findings, and conclusions or recommendations expressed here are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. 

<hr>
<p>Created on July 20, 2003 by <a href="http://www.cs.uic.edu/~liub">Bing Liu.
</body>


